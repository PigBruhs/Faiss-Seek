import requests
from bs4 import BeautifulSoup
import re
import time

# å¸¸é‡å®šä¹‰
BASE_IMAGE_URL = "https://pixnio.com/free-images/"
START_URL = "https://pixnio.com/zh/"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

# æ•°æ®ç»“æ„
image_urls = set()
visited_urls = set()
max_pages = 10
page_count = 0

def crawl_page(url):
    global page_count

    if page_count >= max_pages or url in visited_urls:
        return

    try:
        print(f"[{page_count+1}/{max_pages}] æ­£åœ¨çˆ¬å–: {url}")
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code != 200:
            return

        visited_urls.add(url)
        page_count += 1

        soup = BeautifulSoup(response.text, 'html.parser')

        # æå–å›¾ç‰‡é“¾æ¥
        for img in soup.find_all('img'):
            src = img.get('src')
            if src and src.startswith(BASE_IMAGE_URL):
                image_urls.add(src)

        # æå–ä¸‹ä¸€ä¸ªé¡µé¢é“¾æ¥ï¼Œç»§ç»­é€’å½’ï¼ˆæ·±åº¦ä¼˜å…ˆï¼‰
        for a in soup.find_all('a', href=True):
            href = a['href']
            if href.startswith("https://pixnio.com/zh/") and "?" not in href and href not in visited_urls:
                crawl_page(href)

    except Exception as e:
        print(f"é”™è¯¯è®¿é—®: {url}\né”™è¯¯ä¿¡æ¯: {e}")

# å¯åŠ¨çˆ¬è™«
crawl_page(START_URL)
# ä¿å­˜ç»“æœ
# ä¿å­˜ç»“æœï¼ˆåªä¿å­˜æ–‡ä»¶åéƒ¨åˆ†ï¼‰
save_path = r"e:/è½¯ä»¶å®è®­å¼€å‘/ç­”è€Œå¤šå›¾å›¾/data/pixnio_image_urls.txt"
with open(save_path, "w", encoding="utf-8") as f:
    for url in sorted(image_urls):
        filename = url.split("/")[-1]
        f.write(filename + "\n")


print(f"\nâœ… å…±çˆ¬å–é¡µé¢æ•°: {page_count}ï¼Œå›¾ç‰‡é“¾æ¥æ•°: {len(image_urls)}")
print("ğŸ“„ é“¾æ¥å·²ä¿å­˜è‡³ pixnio_image_urls.txt")